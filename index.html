<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html401">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link href="./Botao_files/main.css" rel="stylesheet" type="text/css">

<style>
body {
  margin-top: 30px;
  margin-bottom: 30px;
  margin-left: 50px;
  margin-right: 50px;
}
a {text-decoration : none; color : #003399;}
a:hover {text-decoration:underline; color: #8C1515; }
</style>

<style>
.disabled{
    pointer-events: none;
    color:black;
}
</style>

<script>
function copy(dest, source) {
  if(dest.source == source) {
    dest.innerHTML = "";
    dest.source = null;
  }
  else {
    dest.innerHTML = source.innerHTML;
    dest.source = source;
  }
  dest.blur();
}
</script>
  
    <title>Botao Hao</title>
    <link rel="stylesheet" type="text/css" href="./Botao_files/html-style.css">
    <meta name="author" content="Botao Hao">
    <meta name="copyright" content="© 2021 Botao Hao">
    <meta http-equiv="Content-language" content="en">
    
    <meta name="description" content="Home page of Botao Hao">
    <meta name="keywords" content="Botao Hao">
    <meta name="keywords" lang="es" content="Botao Hao">
    <meta name="robots" content="all">
  </head>

  <body data-new-gr-c-s-check-loaded="14.1014.0" data-gr-ext-installed="">

    <table border="0" cellspacing="0" cellpadding="20">
      <tbody>
	<tr>
	  <td align="left"><img width="300" height="300" border="0" src="./Botao_files/my_page_photo.png" alt="Botao Hao" title="Botao Hao"></td>
	  <!--<td width="20">&nbsp;</td>-->
	  <!--<td align="left" nowrap>-->
    <td nowrap="" width="500">
	    <h2>Botao Hao (郝博韬)</h2>
		
	    Research Scientist <br>
	    Deepmind, London, UK <br>
		<br>
		
            Email: haobotao000@gmail.com <br>
		<a href="https://www.semanticscholar.org/author/Botao-Hao/30889699">[Semantic Scholar]</a>
		<a href="https://www.linkedin.com/in/botao-hao-23833193/">[Linkedin]</a>
        <a href="CV_Botao_Hao.pdf">[CV]</a> 
		<br>



                <h3>News</h3>
                <div style="width:100%; height:90px; overflow-y:scroll;"><table width="100%">
                  <tbody>
					  <tr>
					                      <td width="100px"><calent> June-2021 </calent></td>
					                      <td><calent> <a href="Talk_Slides_Sparse_Linear_bandits.pdf">Slides</a> available to summarize my research on sparse linear bandits and online sparse RL. </calent></td>
					                    </tr>
					  
					  <tr>
                    <td width="100px"><calent> May-2021 </calent></td>
                    <td><calent> Two papers accepted at ICML 2021 about batch RL. </calent></td>
                  </tr>
				  
				 
    
                </tbody></table></div>

	  </td>
<!--<td><p>This is the paradox of evolution: even if our pain is useful, the urge to escape from the pain remains the most powerful instinct of all. </p></td>-->
	</tr>
<!--        <tr>
          <td align="left"><a href="wang.pdf">Here</a> is a picture made by Seyedshams Feyzabadi for my thesis defense. </td>
        </tr>-->
      </tbody>
    </table>

<div class="content">
<a name="aboutme" class="disabled"><h3>About Me</h3></a>
<p>
</p>
<p>I am a Research Scientist at <a href="https://deepmind.com/">Deepmind</a> in London. Previously, I was a Postdoctoral Researcher in the <a href="https://ee.princeton.edu/">Department of Electrical Engineering</a> at <a href="https://www.princeton.edu/">Princeton University</a> working with Prof. <a href="https://mwang.princeton.edu/">Mengdi Wang</a>. I got my Ph.D. (2014-2019) in the Department of Statistics at Purdue University advised by Prof. <a href="http://www.stat.ucla.edu/~guangcheng/">Guang Cheng</a>.	I was a research intern at <a href="https://deepmind.com/">Deepmind</a> in London mentored by Dr. <a href="https://tor-lattimore.com/">Tor Lattimore</a> and Prof. <a href="https://sites.ualberta.ca/~szepesva/">Csaba Szepesvári</a> (Summer, 2019), and a research intern at <a href="https://research.adobe.com/">Adobe Research</a> in San Jose mentored by Dr. <a href="http://www.zheng-wen.com/index.php">Zheng Wen</a> (Summer, 2018).
</p>

<p> My research interests focus on the statistical perspective of reinforcement learning and multi-armed bandits. Previously, I worked on statistical machine learning and tensor learning.
</p>
</div>


<div class="content">
<a name="publications" class="disabled"><h3>Publications</h3></a>

<p>2022</p>
<ul>
	
<li><p> 
<strong>Contextual Information-Directed Sampling</strong><br>
<b>Botao Hao</b>, Tor Lattimore, Chao Qin <br>
<em>Submitted. </em>
</p></li>

<li><p> 
<strong>Interacting Contour Stochastic Gradient Langevin Dynamics</strong><br>
Wei Deng, Siqi Liang, <b>Botao Hao</b>, Guang Lin, Faming Liang<br>
<em>ICLR 2022. </em>
[<a href="https://openreview.net/forum?id=IK9ap6nxXr2">OpenReview</a>]
</p></li>	
	
<li><p> 
<strong>Confident Least Square Value Iteration with Local Access to a Simulator</strong><br>
<b>Botao Hao</b>, Nevena Lazic, Dong Yin, Yasin Abbasi-Yadkori, Csaba Szepesvári<br>
<em>AISTATS 2022. </em> [<a href="https://proceedings.mlr.press/v151/hao22a.html">Proceedings</a>]
</p></li>	
	
<li><p> 
<strong>Efficient Local Planning with Linear Function Approximation</strong><br>
Dong Yin, <b>Botao Hao</b>, Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvári<br>
<em>ALT 2022. </em>
[<a href="https://arxiv.org/abs/2108.05533">arXiv</a>]
</p></li>

</ul>

<p>2021</p>

<ul>

<li><p> 
<strong>The Neural Testbed: Evaluating Predictive Distributions</strong><br>
Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, <b>Botao Hao</b>, Morteza Ibrahimi, Dieterich Lawson, Xiuyuan Lu, Brendan O'Donoghue, Benjamin Van Roy<br>
<em>Submitted. </em>
[<a href="https://arxiv.org/abs/2110.04629">arXiv</a>][<a href="https://github.com/deepmind/neural_testbed">GitHub</a>]
</p></li>


<li><p> 
<strong>Bandit Phase Retrieval</strong><br>
Tor Lattimore, <b>Botao Hao</b> <br>
<em>NeurIPS 2021. </em>
[<a href="https://openreview.net/forum?id=fThfMoV7Ri">OpenReview</a>]
</p></li>

<li><p> 
<strong>Information Directed Sampling for Sparse Linear Bandits</strong><br>
<b>Botao Hao</b>, Tor Lattimore, Wei Deng <br>
<em>NeurIPS 2021 <strong style="color: red;">(spotlight)</strong>.
 </em>
[<a href="https://proceedings.neurips.cc/paper/2021/hash/8ba6c657b03fc7c8dd4dff8e45defcd2-Abstract.html">Proceedings</a>]
[<a href="NeurIPS2021_Sparse_IDS_slides.pdf">slides</a>]
</p></li>


<li><p> 
<strong>Bootstrapping Fitted Q-Evaluation for Off-Policy Inference</strong><br>
<b>Botao Hao</b>, Xiang Ji, Yaqi Duan, Hao Lu, Csaba Szepesvári, Mengdi Wang <br>
<em>ICML 2021. </em>
[<a href="https://arxiv.org/abs/2102.03607">arXiv</a>]
</p></li>

<li><p> 
<strong>Sparse Feature Selection Makes Batch Reinforcement Learning
More Sample Efficient</strong><br>
<b>Botao Hao</b>, Yaqi Duan, Tor Lattimore, Csaba Szepesvári, Mengdi Wang <br>
<em>ICML 2021. </em>
[<a href="https://arxiv.org/abs/2011.04019">arXiv</a>]
</p></li>

<li><p> 
<strong>Online Sparse Reinforcement Learning</strong><br>
<b>Botao Hao</b>, Tor Lattimore, Csaba Szepesvári, Mengdi Wang <br>
<em>AISTATS 2021. </em>
[<a href="https://arxiv.org/pdf/2011.04018.pdf">arXiv</a>]
[<a href="AISTATS2021_SparseRL_Poster.pdf">poster</a>]
</p></li>

<li><p> 
<strong>Adaptive Approximate Policy Iteration</strong><br>
<b>Botao Hao</b>, Nevena Lazic, Yasin Abbasi-Yadkori, Pooria Joulani, Csaba Szepesvári<br>
<em>AISTATS 2021. </em>
[<a href="https://arxiv.org/pdf/2002.03069.pdf">arXiv</a>]
[<a href="AISTATS2021_AAPI_Poster.pdf">poster</a>]
</p></li>

<li><p> 
<strong>Sparse Tensor Additive Regression</strong><br>
<b>Botao Hao</b>, Boxiang Wang, Pengyuan Wang, Jingfei Zhang, Jian Yang, Will Wei Sun<br>
<em>Journal of Machine Learning Research. </em>
[<a href="https://arxiv.org/abs/1904.00479">arXiv</a>]
</p></li>

</ul>

<p>2020</p>
<ul>
	
<li><p> 
<strong>Tensors in Modern Statistical Learning</strong><br>
Will Wei Sun, <b>Botao Hao</b>, Lexin Li<br>
<em>In Wiley StatsRef: Statistics Reference Online. </em>
[<a href="https://onlinelibrary.wiley.com/doi/10.1002/9781118445112.stat08319">Wiley Online Library</a>]
</p></li>	
	
<li><p> 
<strong>High-Dimensional Sparse Linear Bandits</strong><br>
<b>Botao Hao</b>, Tor Lattimore,  Mengdi Wang<br>
<em>NeurIPS 2020. </em>
[<a href="https://arxiv.org/pdf/2011.04020.pdf">arXiv</a>]
[<a href="NeurIPS2020_HD_Bandits_Slides.pdf">slides</a>]
[<a href="NeurIPS2020_HD_Bandits_Poster.pdf">poster</a>]
</p></li>

<li><p> 
<strong>Adaptive Exploration in Linear Contextual Bandit</strong><br>
<b>Botao Hao</b>, Tor Lattimore,  Csaba Szepesvári<br>
<em>AISTATS 2020. </em>
[<a href="https://arxiv.org/abs/1910.06996">arXiv</a>]
[<a href="adaptive_exploration.pdf">slides</a>]
</p></li>

<li><p> 
<strong>Sparse and Low-rank Tensor Estimation via Cubic Sketchings</strong><br>
<b>Botao Hao</b>, Anru Zhang, Guang Cheng<br>
<em>IEEE Transactions on Information Theory </em>
[<a href="https://arxiv.org/abs/1801.09326">arXiv</a>]
[<a href="fbdc.pdf">slides</a>]
<br>
<em>Accepted in part to AISTATS 2020.</em>
</p></li></ul>

<p>2019</p>
<ul>

<li><p> 
<strong>Nonparametric Bayesian Aggregation for Massive Data</strong><br>
Zuofeng Shang, <b>Botao Hao</b>, Guang Cheng<br>
<em>Journal of Machine Learning Research. </em>
[<a href="http://www.jmlr.org/papers/v20/17-641.html">pdf</a>]
</p></li>

<li><p> 
<strong>Bootstrapping Upper Confidence Bound</strong><br>
<b>Botao Hao</b>, Yasin Abbasi-Yadkori, Zheng Wen, Guang Cheng<br>
<em>NeurIPS 2019. </em>
[<a href="https://arxiv.org/abs/1906.05247">arXiv</a>]
[<a href="poster_nips.pdf">poster</a>]
</p></li>

</ul>


<p>2018</p>
<ul>

<li><p> 
<strong>Simultaneous Clustering and Estimation of Heterogeneous Graphical Models</strong><br>
<b>Botao Hao</b>, Will Wei Sun, Yufeng Liu, Guang Cheng<br>
<em>Journal of Machine Learning Research. </em>
[<a href="http://www.jmlr.org/papers/v18/17-019.html">pdf</a>]
[<a href="SLDS_talk.pdf">slides</a>]
</p></li>

</ul>

</div>



<div class="content">
<a name="service" class="disabled"><h3>Professional Service</h3></a>
<ul>
	<li>
Session chair for Machine Learning/Intelligence Cluster at 2020 INFORMS annual meeting (online)	
</li>	
<li>
Co-chair of Ubicomp 2019 Workshop on Continual and Multimodal Learning for Internet of Things, London, UK
</li>
  <li>
  Reviewers for Journals: Annals of Statistics, Journal of the American Statistical Association, Journal of Machine Learning Research, Journal of Multivariate Analysis, Computational Statistics and Data Analysis, The IEEE Control Systems Letters, Statistica Sinica, Biostatistics, IEEE Journal on Selected Areas in Information Theory, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Information Theory, Artificial Intelligence</li>
  <li>Reviewers for Conferences: SODA 2022, AAAI 2021-2022, ICLR 2021-2022, NeurIPS 2019-2021, AISTATS 2020-2022, ICML 2020-2022, UAI 2020-2021
  </li>
</ul>
</div>

<hr>


<script type="text/JavaScript" language="JavaScript">
//
// format date as dd-mmm-yy
// example: 12-Jan-99
//
function date_ddmmmyy(date)
{
  var d = date.getDate();
  var m = date.getMonth() + 1;
  var y = date.getYear();

  // handle different year values
  // returned by IE and NS in
  // the year 2000.
  if(y >= 2000)
  {
    y -= 2000;
  }
  if(y >= 100)
  {
    y -= 100;
  }

  // could use splitString() here
  // but the following method is
  // more compatible
  var mmm =
    ( 1==m)?'Jan':( 2==m)?'Feb':(3==m)?'Mar':
    ( 4==m)?'Apr':( 5==m)?'May':(6==m)?'Jun':
    ( 7==m)?'Jul':( 8==m)?'Aug':(9==m)?'Sep':
    (10==m)?'Oct':(11==m)?'Nov':'Dec';

  return "" +
    (d<10?"0"+d:d) + "-" +
    mmm + "-" +
    (y<10?"0"+y:y);
}


//
// get last modified date of the
// current document.
//
function date_lastmodified()
{
  var lmd = document.lastModified;
  var s   = "Unknown";
  var d1;

  // check if we have a valid date
  // before proceeding
  if(0 != (d1=Date.parse(lmd)))
  {
    s = "" + date_ddmmmyy(new Date(d1));
  }

  return s;
}

//
// finally display the last modified date
// as DD-MMM-YY
//
document.write("This page was updated on " + date_lastmodified() + ".");

//
</script>
</body></html>